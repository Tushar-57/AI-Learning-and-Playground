# Word Embeddings with Gensim 🌟

Hey! 👋 I’m diving into NLP with Gensim, and this repo is my playground for word embeddings—turning words into vectors that machines can vibe with. Want to explore with me? Let’s learn and maybe wow some Employers too! 😄

---

## What’s the Deal? 🧐
- **Goal**: Play with pre-trained models, train our own, visualize vectors, and test how good they are.
- **Tools**: Gensim, Pandas, Matplotlib, Scikit-learn.
- **Data**: News goodies from `News_Category_Dataset_v3.json`.
- **Models**: Word2Vec, FastText, GloVe + custom ones we’ll build!

---

## What’s Inside? 📖
- **`word_embeddings_with_gensim.ipynb`**: All the action—code, notes, and visuals.
- **Files Needed**: `News_Category_Dataset_v3.json` & `wordsim353crowd.csv` (grab ‘em below!).

---

## How to Dive In 🏊‍♂️
1. Clone it: `git clone <repo-url>`
2. Open `word_embeddings_with_gensim.ipynb` in Colab or Jupyter.
3. Upload `News_Category_Dataset_v3.json` and `wordsim353crowd.csv`.
4. Run the cells and watch the magic! ✨

> **Pro Tip**: In Colab, drop files in `/content/` and you’re set!

---

## Cool Finds 🏆
- **Relationships**: Custom models spot word connections—like “king” and “queen” chilling together.
- **Visuals**: t-SNE and PCA make word clusters pop! 🎨
- **Scores**: Spearman says we’re pretty close to human vibes. Not bad! 📈

---

## Fun Ideas to Try 💡
- Tweak model settings—bigger vectors, crazier results?
- Go interactive with Plotly plots?
- Compare with BERT—how do we stack up?

---

## Why This Exists 🌈
I am trying to learn and share the journey, and maybe catch an employer’s eye. If you’re curious or learning too, let’s geek out together! 😊

---

## Say Hi! 🤝
Got thoughts? Ideas? Drop me a line—I’d love to chat NLP and tweak this with you!

---

## License
MIT License—free to use and remix!